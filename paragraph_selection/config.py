import argparse


def set_args():
    parser = argparse.ArgumentParser()
    # parser.add_argument("--input_path", type=str, defaut='input')
    parser.add_argument("--ckpt_path",
                        type=str,
                        default='checkpoints/para_50000_200_Evaluation_epoch0_ckpt_loss0.08063447918765797.bin')
    parser.add_argument("--premodel_path",
                        type=str,
                        default='../models/para_10000_500_warm_best')
    parser.add_argument("--gra",
                        type=str,
                        default='para')
    # parser.add_argument("--output_path", type=str)
    parser.add_argument("--train_data",
                        default="../data/para.train.json",
                        type=str)
    parser.add_argument("--dev_data",
                        default="../data/version2.para.search.dev.json",
                        type=str)
    parser.add_argument("--train_num",
                        default=200000,
                        type=int)
    parser.add_argument("--dev_num",
                        default=1000,
                        type=int)
    parser.add_argument("--test_num",
                        default=1000,
                        type=int)

    parser.add_argument("--split", type=str, default='dev')
    parser.add_argument("--name",
                        default="Evaluation",
                        type=str)
    parser.add_argument("--data_dir",
                        default="/home/yunxuanxiao/xyx/data/HotpotQA",
                        type=str,
                        help="The input data dir. Should contain the .tsv files (or other data files) for the task.")
    parser.add_argument("--bert_model", default='bert-base-multilingual-cased', type=str,
                        help="Bert pre-trained model selected in the list: bert-base-uncased, "
                             "bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.")
    parser.add_argument("--output_dir",
                        default="predictions",
                        type=str,
                        help="The output directory where the model predictions and checkpoints will be written.")
    parser.add_argument("--ckpt_dir",
                        default="checkpoints",
                        type=str,
                        help="The output directory where the model checkpoints will be written.")
    parser.add_argument("--max_seq_length",
                        default=200,
                        type=int,
                        help="The maximum total input sequence length after WordPiece tokenization. \n"
                             "Sequences longer than this will be truncated, and sequences shorter \n"
                             "than this will be padded.")
    parser.add_argument("--do_train",
                        default=True,
                        action='store_true',
                        help="Whether to run training.")
    parser.add_argument("--do_eval",
                        default=False,
                        action='store_true',
                        help="Whether to run eval on the dev set.")
    parser.add_argument("--do_lower_case",
                        default=False,
                        action='store_true',
                        help="Set this flag if you are using an uncased model.")
    parser.add_argument("--train_batch_size",
                        default=56,
                        type=int,
                        help="Total batch size for training.")
    parser.add_argument("--eval_batch_size",
                        default=28,
                        type=int,
                        help="Total batch size for eval.")
    parser.add_argument("--learning_rate",
                        default=1e-5,
                        type=float,
                        help="The initial learning rate for Adam.")
    parser.add_argument("--num_train_epochs",
                        default=20.0,
                        type=float,
                        help="Total number of training epochs to perform.")
    parser.add_argument("--warmup_proportion",
                        default=0.1,
                        type=float,
                        help="Proportion of training to perform linear learning rate warmup for. "
                             "E.g., 0.1 = 10%% of training.")
    parser.add_argument("--no_cuda",
                        default=False,
                        action='store_true',
                        help="Whether not to use CUDA when available")
    parser.add_argument("--local_rank",
                        type=int,
                        default=-1,
                        help="local_rank for distributed training on gpus")
    parser.add_argument('--seed',
                        type=int,
                        default=42,
                        help="random seed for initialization")
    parser.add_argument('--gradient_accumulation_steps',
                        type=int,
                        default=2,
                        help="Number of updates steps to accumulate before performing a backward/update pass.")
    parser.add_argument('--fp16',
                        default=False,
                        action='store_true',
                        help="Whether to use 16-bit float precision instead of 32-bit")
    parser.add_argument('--loss_scale',
                        type=float, default=0,
                        help="Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\n"
                             "0 (default value): dynamic loss scaling.\n"
                             "Positive power of 2: static loss scaling value.\n")

    args = parser.parse_args()
    return args
